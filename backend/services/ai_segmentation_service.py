"""
üß† CereBloom - Service IA de Segmentation
Int√©gration de votre mod√®le U-Net Kaggle pour la segmentation de tumeurs c√©r√©brales
"""

import os
import uuid
import numpy as np
import nibabel as nib
# TensorFlow import avec gestion d'erreur pour Python 3.13
try:
    import tensorflow as tf
    TENSORFLOW_AVAILABLE = True
    print("‚úÖ TensorFlow disponible")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("‚ö†Ô∏è TensorFlow non disponible - Mode simulation activ√©")
    # Cr√©er un mock TensorFlow pour √©viter les erreurs
    class MockTF:
        class keras:
            class models:
                @staticmethod
                def load_model(*args, **kwargs):
                    return None
            class backend:
                @staticmethod
                def epsilon():
                    return 1e-7
    tf = MockTF()

import cv2
try:
    from sklearn.preprocessing import MinMaxScaler
except ImportError:
    print("‚ö†Ô∏è scikit-learn non disponible")
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from config.database import get_database
from config.settings import settings, TUMOR_SEGMENT_COLORS
from models.database_models import (
    AISegmentation, TumorSegment, VolumetricAnalysis,
    ImageSeries, MedicalImage, Patient, Doctor,
    SegmentationStatus, TumorType
)
from services.mlops_service import mlops_service

logger = logging.getLogger(__name__)

# Configuration du mod√®le (import√©e de loadmodel.py)
IMG_SIZE = 128
VOLUME_SLICES = 100
VOLUME_START_AT = 22

# Classification m√©dicale des r√©gions tumorales selon BraTS
TUMOR_CLASSES = {
    0: {'name': 'Tissu sain', 'abbr': 'Normal', 'color': '#000000', 'alpha': 0.0},
    1: {'name': 'Noyau n√©crotique/kystique', 'abbr': 'Necrotic Core', 'color': '#FF0000', 'alpha': 0.8},
    2: {'name': '≈íd√®me p√©ritumoral', 'abbr': 'Peritumoral Edema', 'color': '#00FF00', 'alpha': 0.7},
    3: {'name': 'Tumeur rehauss√©e', 'abbr': 'Enhancing Tumor', 'color': '#0080FF', 'alpha': 0.9}
}

class AISegmentationService:
    """Service de segmentation IA avec votre mod√®le U-Net Kaggle"""

    def __init__(self):
        self.model_path = settings.AI_MODEL_PATH
        self.model_version = settings.AI_MODEL_VERSION
        self.confidence_threshold = settings.AI_CONFIDENCE_THRESHOLD
        self.processing_timeout = settings.AI_PROCESSING_TIMEOUT
        self.model = None
        self.executor = ThreadPoolExecutor(max_workers=2)

        # M√©triques personnalis√©es pour votre mod√®le
        self.custom_objects = {
            'dice_coef': self.dice_coef,
            'precision': self.precision,
            'sensitivity': self.sensitivity,
            'specificity': self.specificity,
            'dice_coef_necrotic': self.dice_coef_necrotic,
            'dice_coef_edema': self.dice_coef_edema,
            'dice_coef_enhancing': self.dice_coef_enhancing
        }

    async def load_model(self):
        """Charge votre mod√®le U-Net Kaggle"""
        try:
            if self.model is None:
                logger.info(f"Chargement du mod√®le: {self.model_path}")

                # Chargement asynchrone du mod√®le
                loop = asyncio.get_event_loop()
                self.model = await loop.run_in_executor(
                    self.executor,
                    self._load_model_sync
                )

                logger.info("‚úÖ Mod√®le U-Net charg√© avec succ√®s")

            return self.model

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise

    def _load_model_sync(self):
        """Charge le mod√®le de mani√®re synchrone"""
        return tf.keras.models.load_model(
            self.model_path,
            custom_objects=self.custom_objects,
            compile=False
        )

    async def create_segmentation(
        self,
        patient_id: str,
        doctor_id: str,
        image_series_id: str,
        input_parameters: Optional[Dict[str, Any]] = None
    ) -> str:
        """Cr√©e une nouvelle segmentation IA avec tracking MLOps"""
        async for db in get_database():
            try:
                # Cr√©ation de l'enregistrement de segmentation
                segmentation_id = str(uuid.uuid4())
                segmentation = AISegmentation(
                    id=segmentation_id,
                    patient_id=patient_id,
                    doctor_id=doctor_id,
                    image_series_id=image_series_id,
                    status=SegmentationStatus.PROCESSING,
                    input_parameters=input_parameters or {},
                    started_at=datetime.utcnow()
                )

                db.add(segmentation)
                await db.commit()

                # üìä MLOPS - D√©marrage du tracking automatique
                mlflow_run_id = mlops_service.start_segmentation_run(
                    patient_id=patient_id,
                    doctor_id=doctor_id,
                    image_series_id=image_series_id,
                    input_parameters=input_parameters
                )

                # Stockage du run_id MLflow pour suivi
                segmentation.input_parameters = {
                    **(input_parameters or {}),
                    "mlflow_run_id": mlflow_run_id
                }
                await db.commit()

                logger.info(f"Segmentation cr√©√©e: {segmentation_id} | MLflow Run: {mlflow_run_id}")

                # Lancement du traitement en arri√®re-plan
                asyncio.create_task(self._process_segmentation(segmentation_id))

                return segmentation_id

            except Exception as e:
                logger.error(f"Erreur lors de la cr√©ation de segmentation: {e}")
                await db.rollback()
                raise

    async def _process_segmentation(self, segmentation_id: str):
        """Traite une segmentation en arri√®re-plan"""
        start_time = datetime.utcnow()

        async for db in get_database():
            try:
                # R√©cup√©ration de la segmentation
                result = await db.execute(
                    select(AISegmentation).where(AISegmentation.id == segmentation_id)
                )
                segmentation = result.scalar_one_or_none()

                if not segmentation:
                    logger.error(f"Segmentation non trouv√©e: {segmentation_id}")
                    return

                # R√©cup√©ration des images
                images_data = await self._load_image_series(db, segmentation.image_series_id)
                if not images_data:
                    await self._update_segmentation_status(db, segmentation_id, SegmentationStatus.FAILED)
                    return

                # Chargement du mod√®le
                model = await self.load_model()

                # Pr√©paration des donn√©es
                input_data = await self._prepare_input_data(images_data)

                # Ex√©cution de la segmentation
                loop = asyncio.get_event_loop()
                segmentation_result = await loop.run_in_executor(
                    self.executor,
                    self._run_segmentation,
                    model,
                    input_data
                )

                # Calcul des volumes et analyse
                volume_analysis = await self._calculate_volumes(segmentation_result)
                tumor_segments = await self._extract_tumor_segments(segmentation_result, volume_analysis)

                # Calcul du temps de traitement
                processing_time_seconds = (datetime.utcnow() - start_time).total_seconds()
                processing_time = str(datetime.utcnow() - start_time)

                # Mise √† jour de la segmentation
                segmentation.status = SegmentationStatus.COMPLETED
                segmentation.completed_at = datetime.utcnow()
                segmentation.processing_time = processing_time
                segmentation.segmentation_results = {
                    "mask_shape": segmentation_result.shape,
                    "output_path": f"uploads/segmentation_results/{segmentation_id}_result.nii"
                }
                segmentation.volume_analysis = volume_analysis
                confidence_score = float(np.mean([seg["confidence_score"] for seg in tumor_segments]))
                segmentation.confidence_score = confidence_score

                # Sauvegarde du masque de segmentation
                await self._save_segmentation_mask(segmentation_result, segmentation_id)

                # Cr√©ation des segments tumoraux
                await self._create_tumor_segments(db, segmentation_id, tumor_segments)

                # Cr√©ation de l'analyse volum√©trique
                await self._create_volumetric_analysis(db, segmentation_id, volume_analysis, tumor_segments)

                # üìà MLOPS - Enregistrement des r√©sultats
                mlflow_run_id = segmentation.input_parameters.get("mlflow_run_id")
                if mlflow_run_id:
                    mlops_service.log_segmentation_results(
                        run_id=mlflow_run_id,
                        processing_time=processing_time_seconds,
                        confidence_score=confidence_score,
                        volume_analysis=volume_analysis,
                        tumor_segments=tumor_segments,
                        status="completed"
                    )

                await db.commit()

                logger.info(f"‚úÖ Segmentation termin√©e: {segmentation_id} | MLOps tracking: {mlflow_run_id}")

            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement de segmentation {segmentation_id}: {e}")

                # üìà MLOPS - Enregistrement de l'erreur
                try:
                    result = await db.execute(
                        select(AISegmentation).where(AISegmentation.id == segmentation_id)
                    )
                    segmentation = result.scalar_one_or_none()
                    if segmentation:
                        mlflow_run_id = segmentation.input_parameters.get("mlflow_run_id")
                        if mlflow_run_id:
                            mlops_service.log_error(
                                run_id=mlflow_run_id,
                                error_message=str(e),
                                error_type="segmentation_processing_error"
                            )
                except Exception as mlops_error:
                    logger.error(f"‚ùå Erreur lors de l'enregistrement MLOps: {mlops_error}")

                await self._update_segmentation_status(db, segmentation_id, SegmentationStatus.FAILED)
                await db.rollback()

    async def _load_image_series(self, db: AsyncSession, image_series_id: str) -> Optional[Dict[str, np.ndarray]]:
        """Charge une s√©rie d'images m√©dicales"""
        try:
            # R√©cup√©ration de la s√©rie d'images
            result = await db.execute(
                select(ImageSeries).where(ImageSeries.id == image_series_id)
            )
            image_series = result.scalar_one_or_none()

            if not image_series:
                logger.error(f"S√©rie d'images non trouv√©e: {image_series_id}")
                return None

            # R√©cup√©ration des images individuelles
            image_ids = image_series.image_ids
            if not image_ids or len(image_ids) < 4:
                logger.error(f"S√©rie d'images incompl√®te: {len(image_ids) if image_ids else 0} images")
                return None

            images_data = {}
            for image_id in image_ids:
                result = await db.execute(
                    select(MedicalImage).where(MedicalImage.id == image_id)
                )
                image = result.scalar_one_or_none()

                if image:
                    # Chargement de l'image NIfTI
                    img_data = nib.load(image.file_path).get_fdata()
                    images_data[image.modality.value] = img_data

            # V√©rification que nous avons les 4 modalit√©s requises
            required_modalities = ["T1", "T1CE", "T2", "FLAIR"]
            if not all(mod in images_data for mod in required_modalities):
                logger.error(f"Modalit√©s manquantes. Trouv√©es: {list(images_data.keys())}")
                return None

            logger.info(f"Images charg√©es: {list(images_data.keys())}")
            return images_data

        except Exception as e:
            logger.error(f"Erreur lors du chargement des images: {e}")
            return None

    async def _prepare_input_data(self, images_data: Dict[str, np.ndarray]) -> np.ndarray:
        """
        Pr√©pare les donn√©es d'entr√©e pour votre mod√®le U-Net selon loadmodel.py
        """
        try:
            # R√©cup√©ration des modalit√©s dans l'ordre requis
            t1 = images_data["T1"]
            t1ce = images_data["T1CE"]
            t2 = images_data["T2"]
            flair = images_data["FLAIR"]

            logger.info(f"Formes originales - T1: {t1.shape}, T1CE: {t1ce.shape}, T2: {t2.shape}, FLAIR: {flair.shape}")

            # Normalisation standardis√©e (percentile-based pour √©viter les outliers)
            normalized_data = {}
            for modality, data in [('t1', t1), ('t1ce', t1ce), ('t2', t2), ('flair', flair)]:
                # Normalisation robuste
                p1, p99 = np.percentile(data[data > 0], [1, 99])
                normalized = np.clip((data - p1) / (p99 - p1), 0, 1)
                normalized_data[modality] = normalized

            # Pr√©paration pour le mod√®le (FLAIR + T1CE comme entr√©es principales selon loadmodel.py)
            X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))

            for slice_idx in range(VOLUME_SLICES):
                z_idx = slice_idx + VOLUME_START_AT
                # V√©rifier que l'index est dans les limites
                if z_idx < normalized_data['flair'].shape[2]:
                    X[slice_idx, :, :, 0] = cv2.resize(normalized_data['flair'][:, :, z_idx], (IMG_SIZE, IMG_SIZE))
                    X[slice_idx, :, :, 1] = cv2.resize(normalized_data['t1ce'][:, :, z_idx], (IMG_SIZE, IMG_SIZE))
                else:
                    # Remplir avec des z√©ros si on d√©passe les limites
                    X[slice_idx, :, :, 0] = np.zeros((IMG_SIZE, IMG_SIZE))
                    X[slice_idx, :, :, 1] = np.zeros((IMG_SIZE, IMG_SIZE))

            logger.info(f"‚úÖ Donn√©es d'entr√©e pr√©par√©es selon loadmodel.py: {X.shape}")
            return X

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es: {e}")
            raise

    def _run_segmentation(self, model, input_data: np.ndarray) -> np.ndarray:
        """Ex√©cute la segmentation avec votre mod√®le U-Net"""
        try:
            logger.info("Ex√©cution de la segmentation...")

            # Pr√©diction avec votre mod√®le
            prediction = model.predict(input_data)

            # Post-traitement selon votre mod√®le
            # Suppression de la dimension batch
            prediction = np.squeeze(prediction, axis=0)

            # Application du seuil de confiance
            prediction = (prediction > self.confidence_threshold).astype(np.uint8)

            logger.info(f"Segmentation termin√©e: {prediction.shape}")
            return prediction

        except Exception as e:
            logger.error(f"Erreur lors de la segmentation: {e}")
            raise

    # M√©triques personnalis√©es pour votre mod√®le Kaggle (selon loadmodel.py)
    @staticmethod
    def dice_coef(y_true, y_pred, smooth=1.0):
        """Coefficient de Dice - M√©trique standard en segmentation m√©dicale"""
        from tensorflow.keras import backend as K
        class_num = 4
        total_loss = 0
        for i in range(class_num):
            y_true_f = K.flatten(y_true[:,:,:,i])
            y_pred_f = K.flatten(y_pred[:,:,:,i])
            intersection = K.sum(y_true_f * y_pred_f)
            loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))
            total_loss += loss
        return total_loss / class_num

    @staticmethod
    def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):
        """Dice pour r√©gion n√©crotique - Critique pour planification chirurgicale"""
        from tensorflow.keras import backend as K
        intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))
        return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)

    @staticmethod
    def dice_coef_edema(y_true, y_pred, epsilon=1e-6):
        """Dice pour ≈ìd√®me - Important pour √©valuation de l'effet de masse"""
        from tensorflow.keras import backend as K
        intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))
        return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)

    @staticmethod
    def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):
        """Dice pour tumeur rehauss√©e - Cible th√©rapeutique principale"""
        from tensorflow.keras import backend as K
        intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))
        return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)

    @staticmethod
    def precision(y_true, y_pred):
        """Pr√©cision - Minimise les faux positifs"""
        from tensorflow.keras import backend as K
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        return true_positives / (predicted_positives + K.epsilon())

    @staticmethod
    def sensitivity(y_true, y_pred):
        """Sensibilit√©/Recall - Minimise les faux n√©gatifs (critique en m√©dical)"""
        from tensorflow.keras import backend as K
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        return true_positives / (possible_positives + K.epsilon())

    @staticmethod
    def specificity(y_true, y_pred):
        """Sp√©cificit√© - Capacit√© √† identifier les tissus sains"""
        from tensorflow.keras import backend as K
        true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
        possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
        return true_negatives / (possible_negatives + K.epsilon())

    async def _calculate_volumes(self, segmentation_result: np.ndarray) -> Dict[str, Any]:
        """Calcule les volumes des diff√©rents segments"""
        try:
            # Calcul des volumes en cm¬≥ (en supposant une r√©solution de 1mm¬≥ par voxel)
            voxel_volume = 0.001  # 1mm¬≥ = 0.001 cm¬≥

            # Extraction des diff√©rents segments selon votre mod√®le
            if len(segmentation_result.shape) == 4:
                # Mod√®le multi-classe
                necrotic_volume = np.sum(segmentation_result[:,:,:,0]) * voxel_volume
                edema_volume = np.sum(segmentation_result[:,:,:,1]) * voxel_volume
                enhancing_volume = np.sum(segmentation_result[:,:,:,2]) * voxel_volume
            else:
                # Mod√®le binaire - adaptation n√©cessaire
                total_tumor = np.sum(segmentation_result) * voxel_volume
                # Estimation des sous-segments (√† adapter selon votre mod√®le)
                necrotic_volume = total_tumor * 0.3
                edema_volume = total_tumor * 0.4
                enhancing_volume = total_tumor * 0.3

            total_volume = necrotic_volume + edema_volume + enhancing_volume

            volume_analysis = {
                "total_tumor_volume": float(total_volume),
                "necrotic_core_volume": float(necrotic_volume),
                "peritumoral_edema_volume": float(edema_volume),
                "enhancing_tumor_volume": float(enhancing_volume),
                "voxel_count": int(np.sum(segmentation_result > 0)),
                "analysis_timestamp": datetime.utcnow().isoformat()
            }

            logger.info(f"Volumes calcul√©s: Total={total_volume:.2f} cm¬≥")
            return volume_analysis

        except Exception as e:
            logger.error(f"Erreur lors du calcul des volumes: {e}")
            raise

    async def _extract_tumor_segments(self, segmentation_result: np.ndarray, volume_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extrait les segments tumoraux d√©taill√©s"""
        try:
            total_volume = volume_analysis["total_tumor_volume"]
            segments = []

            # Segment n√©crotique
            if volume_analysis["necrotic_core_volume"] > 0:
                segments.append({
                    "segment_type": TumorType.NECROTIC_CORE,
                    "volume_cm3": volume_analysis["necrotic_core_volume"],
                    "percentage": (volume_analysis["necrotic_core_volume"] / total_volume * 100) if total_volume > 0 else 0,
                    "color_code": TUMOR_SEGMENT_COLORS["NECROTIC_CORE"],
                    "description": "Noyau n√©crotique central",
                    "confidence_score": 0.85,  # √Ä adapter selon votre mod√®le
                    "coordinates": self._extract_coordinates(segmentation_result, 0),
                    "statistical_features": self._calculate_segment_statistics(segmentation_result, 0)
                })

            # ≈íd√®me p√©ritumoral
            if volume_analysis["peritumoral_edema_volume"] > 0:
                segments.append({
                    "segment_type": TumorType.PERITUMORAL_EDEMA,
                    "volume_cm3": volume_analysis["peritumoral_edema_volume"],
                    "percentage": (volume_analysis["peritumoral_edema_volume"] / total_volume * 100) if total_volume > 0 else 0,
                    "color_code": TUMOR_SEGMENT_COLORS["PERITUMORAL_EDEMA"],
                    "description": "≈íd√®me p√©ritumoral",
                    "confidence_score": 0.82,
                    "coordinates": self._extract_coordinates(segmentation_result, 1),
                    "statistical_features": self._calculate_segment_statistics(segmentation_result, 1)
                })

            # Tumeur rehauss√©e
            if volume_analysis["enhancing_tumor_volume"] > 0:
                segments.append({
                    "segment_type": TumorType.ENHANCING_TUMOR,
                    "volume_cm3": volume_analysis["enhancing_tumor_volume"],
                    "percentage": (volume_analysis["enhancing_tumor_volume"] / total_volume * 100) if total_volume > 0 else 0,
                    "color_code": TUMOR_SEGMENT_COLORS["ENHANCING_TUMOR"],
                    "description": "Tumeur avec rehaussement",
                    "confidence_score": 0.88,
                    "coordinates": self._extract_coordinates(segmentation_result, 2),
                    "statistical_features": self._calculate_segment_statistics(segmentation_result, 2)
                })

            logger.info(f"Segments extraits: {len(segments)}")
            return segments

        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des segments: {e}")
            raise

    def _extract_coordinates(self, segmentation_result: np.ndarray, segment_index: int) -> Dict[str, Any]:
        """Extrait les coordonn√©es 3D d'un segment"""
        try:
            if len(segmentation_result.shape) == 4 and segment_index < segmentation_result.shape[3]:
                segment_mask = segmentation_result[:,:,:,segment_index]
            else:
                segment_mask = segmentation_result

            # Recherche des coordonn√©es des voxels actifs
            coords = np.where(segment_mask > 0)

            if len(coords[0]) > 0:
                # Calcul du centro√Øde
                centroid = [float(np.mean(coord)) for coord in coords]

                # Bo√Æte englobante
                bbox = {
                    "min": [int(np.min(coord)) for coord in coords],
                    "max": [int(np.max(coord)) for coord in coords]
                }

                return {
                    "centroid": centroid,
                    "bounding_box": bbox,
                    "voxel_count": len(coords[0])
                }

            return {"centroid": [0, 0, 0], "bounding_box": {"min": [0, 0, 0], "max": [0, 0, 0]}, "voxel_count": 0}

        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des coordonn√©es: {e}")
            return {}

    def _calculate_segment_statistics(self, segmentation_result: np.ndarray, segment_index: int) -> Dict[str, Any]:
        """Calcule les statistiques d'un segment"""
        try:
            if len(segmentation_result.shape) == 4 and segment_index < segmentation_result.shape[3]:
                segment_mask = segmentation_result[:,:,:,segment_index]
            else:
                segment_mask = segmentation_result

            # Statistiques de base
            stats = {
                "mean_intensity": float(np.mean(segment_mask[segment_mask > 0])) if np.any(segment_mask > 0) else 0.0,
                "std_intensity": float(np.std(segment_mask[segment_mask > 0])) if np.any(segment_mask > 0) else 0.0,
                "max_intensity": float(np.max(segment_mask)) if np.any(segment_mask > 0) else 0.0,
                "min_intensity": float(np.min(segment_mask[segment_mask > 0])) if np.any(segment_mask > 0) else 0.0,
                "voxel_count": int(np.sum(segment_mask > 0))
            }

            return stats

        except Exception as e:
            logger.error(f"Erreur lors du calcul des statistiques: {e}")
            return {}

    async def _save_segmentation_mask(self, segmentation_result: np.ndarray, segmentation_id: str):
        """Sauvegarde le masque de segmentation"""
        try:
            output_dir = Path(settings.SEGMENTATION_RESULTS_DIR)
            output_dir.mkdir(parents=True, exist_ok=True)

            output_path = output_dir / f"{segmentation_id}_result.nii"

            # Cr√©ation de l'image NIfTI
            nii_img = nib.Nifti1Image(segmentation_result, affine=np.eye(4))
            nib.save(nii_img, str(output_path))

            logger.info(f"Masque de segmentation sauvegard√©: {output_path}")

        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du masque: {e}")
            raise

    async def _create_tumor_segments(self, db: AsyncSession, segmentation_id: str, tumor_segments: List[Dict[str, Any]]):
        """Cr√©e les enregistrements de segments tumoraux"""
        try:
            for segment_data in tumor_segments:
                segment = TumorSegment(
                    id=str(uuid.uuid4()),
                    segmentation_id=segmentation_id,
                    segment_type=segment_data["segment_type"],
                    volume_cm3=segment_data["volume_cm3"],
                    percentage=segment_data["percentage"],
                    coordinates=segment_data.get("coordinates"),
                    color_code=segment_data["color_code"],
                    description=segment_data["description"],
                    confidence_score=segment_data["confidence_score"],
                    statistical_features=segment_data.get("statistical_features")
                )

                db.add(segment)

            logger.info(f"Segments tumoraux cr√©√©s: {len(tumor_segments)}")

        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation des segments: {e}")
            raise

    async def _create_volumetric_analysis(self, db: AsyncSession, segmentation_id: str, volume_analysis: Dict[str, Any], tumor_segments: List[Dict[str, Any]]):
        """Cr√©e l'analyse volum√©trique"""
        try:
            volumetric_analysis = VolumetricAnalysis(
                id=str(uuid.uuid4()),
                segmentation_id=segmentation_id,
                total_tumor_volume=volume_analysis["total_tumor_volume"],
                necrotic_core_volume=volume_analysis["necrotic_core_volume"],
                peritumoral_edema_volume=volume_analysis["peritumoral_edema_volume"],
                enhancing_tumor_volume=volume_analysis["enhancing_tumor_volume"],
                tumor_burden_index=volume_analysis["total_tumor_volume"] / 1000,  # Index simple
                growth_rate_analysis={"initial_analysis": True}
            )

            db.add(volumetric_analysis)
            logger.info("Analyse volum√©trique cr√©√©e")

        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation de l'analyse volum√©trique: {e}")
            raise

    async def _update_segmentation_status(self, db: AsyncSession, segmentation_id: str, status: SegmentationStatus):
        """Met √† jour le statut d'une segmentation"""
        try:
            result = await db.execute(
                select(AISegmentation).where(AISegmentation.id == segmentation_id)
            )
            segmentation = result.scalar_one_or_none()

            if segmentation:
                segmentation.status = status
                if status == SegmentationStatus.FAILED:
                    segmentation.completed_at = datetime.utcnow()
                await db.commit()

        except Exception as e:
            logger.error(f"Erreur lors de la mise √† jour du statut: {e}")
            await db.rollback()
